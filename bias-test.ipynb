{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b67256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278f326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b568cec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'img/gender-science'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a15d391",
   "metadata": {},
   "source": [
    "## Load the CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b7a53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-L/14\", device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abd771da",
   "metadata": {},
   "source": [
    "## Load image generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cbea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_asset(A='male', X='science'):\n",
    "    path = os.path.join(DATA_PATH, f'{A}-{X}')\n",
    "    images = [preprocess(Image.open(os.path.join(path, f))).to(device) for f in os.listdir(path)]\n",
    "    images = torch.stack(images)\n",
    "    with torch.no_grad():\n",
    "        image_feature = model.encode_image(images)\n",
    "    return image_feature.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d278a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_n = load_image_asset(A='neutral', X='science')\n",
    "S_a = load_image_asset(A='male', A='science')\n",
    "S_b = load_image_asset(A='female', A='science')\n",
    "A_n = load_image_asset(A='neutral', A='art')\n",
    "A_a = load_image_asset(A='male', A='art')\n",
    "A_b = load_image_asset(A='female', X='art')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "317b9850",
   "metadata": {},
   "source": [
    "## Compute association measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d59b538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pairwise_associate_score(S1, S2):\n",
    "    scores = []\n",
    "    for i, j in itertools.product(np.arange(S1.shape[0]), np.arange(S2.shape[0])):\n",
    "        s = distance.cosine(S1[i], S2[j])\n",
    "        scores.append(s)\n",
    "    return np.array(scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6377ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_associate_scores(S1, S2, num_images_per_prompt=5):\n",
    "    # S2 should be larger than S1\n",
    "    scale = S2.shape[0] // S1.shape[0]\n",
    "    \n",
    "    scores = []\n",
    "    for i in range(0, S1.shape[0], num_images_per_prompt):\n",
    "        j = i + num_images_per_prompt\n",
    "        asc = compute_pairwise_associate_score(S1[i:j], S2[i*scale:j*scale])\n",
    "        scores.append(asc)\n",
    "    \n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17180905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_differential_association(X_n, X_a, X_b, Y_n, Y_a, Y_b):\n",
    "    \n",
    "    X_pos = get_associate_scores(X_n, X_a)\n",
    "    X_neg = get_associate_scores(X_n, X_b)\n",
    "\n",
    "    Y_pos = get_associate_scores(Y_n, Y_a)\n",
    "    Y_neg = get_associate_scores(Y_n, Y_b)\n",
    "    \n",
    "    print(X_pos.mean(), X_neg.mean(), Y_pos.mean(), Y_neg.mean())\n",
    "    \n",
    "    return (X_pos.mean() - X_neg.mean()) - (Y_pos.mean() - Y_neg.mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044947ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# differential association\n",
    "get_differential_association(S_n, S_a, S_b, A_n, A_a, A_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24d0e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_effect_size(X_n, X_a, X_b, Y_n, Y_a, Y_b):\n",
    "    X_pos = get_associate_scores(X_n, X_a)\n",
    "    X_neg = get_associate_scores(X_n, X_b)\n",
    "    X = X_pos - X_neg\n",
    "    \n",
    "    Y_pos = get_associate_scores(Y_n, Y_a)\n",
    "    Y_neg = get_associate_scores(Y_n, Y_b)\n",
    "    Y = Y_pos - Y_neg\n",
    "    \n",
    "    return (X.mean() - Y.mean()) / np.concatenate((X, Y)).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa8a789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# effect size\n",
    "get_effect_size(S_n, S_a, S_b, A_n, A_a, A_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
